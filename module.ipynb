{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a3ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## module file for 36122 final project\n",
    "## contains internal and external python modules\n",
    "## functions and classes designed for the project\n",
    "import numpy as np\n",
    "from newsapi import NewsApiClient\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "from IPython.display import Image\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d8e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def headlines(source=None,category=None):\n",
    "    ## function to pull top headlines; user can choose a specific news source \n",
    "    ## or a news category, but not both\n",
    "    api_key='6adfdd5a094c46daa53ea3bbb74379a5'\n",
    "    api=NewsApiClient(api_key=api_key)\n",
    "    if source is not None:\n",
    "        result=api.get_top_headlines(sources=source)\n",
    "    elif category is not None:\n",
    "        result=api.get_top_headlines(category=category)\n",
    "    else:\n",
    "        result=api.get_top_headlines()\n",
    "    return result['articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c07865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_id_matcher(article):\n",
    "\n",
    "    news_sources_names=['ABC News', 'ABC News (AU)', 'Aftenposten', 'Al Jazeera English', 'ANSA.it', 'Argaam', 'Ars Technica', 'Ary News', 'Associated Press', 'Australian Financial Review', 'Axios', 'BBC News', 'BBC Sport', 'Bild', 'Blasting News (BR)', 'Bleacher Report', 'Bloomberg', 'Breitbart News', 'Business Insider', 'Buzzfeed', 'CBC News', 'CBS News', 'CNN', 'CNN Spanish', 'Crypto Coins News', 'Der Tagesspiegel', 'Die Zeit', 'El Mundo', 'Engadget', 'Entertainment Weekly', 'ESPN', 'ESPN Cric Info', 'Financial Post', 'Focus', 'Football Italia', 'Fortune', 'FourFourTwo', 'Fox News', 'Fox Sports', 'Globo', 'Google News', 'Google News (Argentina)', 'Google News (Australia)', 'Google News (Brasil)', 'Google News (Canada)', 'Google News (France)', 'Google News (India)', 'Google News (Israel)', 'Google News (Italy)', 'Google News (Russia)', 'Google News (Saudi Arabia)', 'Google News (UK)', 'Göteborgs-Posten', 'Gruenderszene', 'Hacker News', 'Handelsblatt', 'IGN', 'Il Sole 24 Ore', 'Independent', 'Infobae', 'InfoMoney', 'La Gaceta', 'La Nacion', 'La Repubblica', 'Le Monde', 'Lenta', \"L'equipe\", 'Les Echos', 'Libération', 'Marca', 'Mashable', 'Medical News Today', 'MSNBC', 'MTV News', 'MTV News (UK)', 'National Geographic', 'National Review', 'NBC News', 'News24', 'New Scientist', 'News.com.au', 'Newsweek', 'New York Magazine', 'Next Big Future', 'NFL News', 'NHL News', 'NRK', 'Politico', 'Polygon', 'RBC', 'Recode', 'Reddit /r/all', 'Reuters', 'RT', 'RTE', 'RTL Nieuws', 'SABQ', 'Spiegel Online', 'Svenska Dagbladet', 'T3n', 'TalkSport', 'TechCrunch', 'TechCrunch (CN)', 'TechRadar', 'The American Conservative', 'The Globe And Mail', 'The Hill', 'The Hindu', 'The Huffington Post', 'The Irish Times', 'The Jerusalem Post', 'The Lad Bible', 'The Next Web', 'The Sport Bible', 'The Times of India', 'The Verge', 'The Wall Street Journal', 'The Washington Post', 'The Washington Times', 'Time', 'USA Today', 'Vice News', 'Wired', 'Wired.de', 'Wirtschafts Woche', 'Xinhua Net', 'Ynet']\n",
    "    news_sources_ids=['abc-news', 'abc-news-au', 'aftenposten', 'al-jazeera-english', 'ansa', 'argaam', 'ars-technica', 'ary-news', 'associated-press', 'australian-financial-review', 'axios', 'bbc-news', 'bbc-sport', 'bild', 'blasting-news-br', 'bleacher-report', 'bloomberg', 'breitbart-news', 'business-insider', 'buzzfeed', 'cbc-news', 'cbs-news', 'cnn', 'cnn-es', 'crypto-coins-news', 'der-tagesspiegel', 'die-zeit', 'el-mundo', 'engadget', 'entertainment-weekly', 'espn', 'espn-cric-info', 'financial-post', 'focus', 'football-italia', 'fortune', 'four-four-two', 'fox-news', 'fox-sports', 'globo', 'google-news', 'google-news-ar', 'google-news-au', 'google-news-br', 'google-news-ca', 'google-news-fr', 'google-news-in', 'google-news-is', 'google-news-it', 'google-news-ru', 'google-news-sa', 'google-news-uk', 'goteborgs-posten', 'gruenderszene', 'hacker-news', 'handelsblatt', 'ign', 'il-sole-24-ore', 'independent', 'infobae', 'info-money', 'la-gaceta', 'la-nacion', 'la-repubblica', 'le-monde', 'lenta', 'lequipe', 'les-echos', 'liberation', 'marca', 'mashable', 'medical-news-today', 'msnbc', 'mtv-news', 'mtv-news-uk', 'national-geographic', 'national-review', 'nbc-news', 'news24', 'new-scientist', 'news-com-au', 'newsweek', 'new-york-magazine', 'next-big-future', 'nfl-news', 'nhl-news', 'nrk', 'politico', 'polygon', 'rbc', 'recode', 'reddit-r-all', 'reuters', 'rt', 'rte', 'rtl-nieuws', 'sabq', 'spiegel-online', 'svenska-dagbladet', 't3n', 'talksport', 'techcrunch', 'techcrunch-cn', 'techradar', 'the-american-conservative', 'the-globe-and-mail', 'the-hill', 'the-hindu', 'the-huffington-post', 'the-irish-times', 'the-jerusalem-post', 'the-lad-bible', 'the-next-web', 'the-sport-bible', 'the-times-of-india', 'the-verge', 'the-wall-street-journal', 'the-washington-post', 'the-washington-times', 'time', 'usa-today', 'vice-news', 'wired', 'wired-de', 'wirtschafts-woche', 'xinhua-net', 'ynet']\n",
    "    ## id and name information of all news sources scalped from newsapi and stored as lists\n",
    "    \n",
    "    for i in range(len(news_sources_names)):\n",
    "        name=news_sources_names[i]\n",
    "        ID=news_sources_ids[i]\n",
    "        if article['source']['name']==name:\n",
    "            article['source']['id']=ID\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e373f6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_matcher(article):\n",
    "    ##  this function is designed to fulfill incomplete information in API response. \n",
    "    ## I/O is an article information dictionary that is the standard output of newsapi module \n",
    "    response=requests.get(article['url'])\n",
    "    ## check if full article could be retrieved from the website\n",
    "    code=response.status_code\n",
    "    if str(code)[0]!='2':\n",
    "        raise Exception('Access Denied')\n",
    "        \n",
    "    html=response.text\n",
    "    page=BeautifulSoup(html,'lxml')\n",
    "    metas=page.select('meta')\n",
    "    author=''\n",
    "    description=''\n",
    "    title=page.title.text\n",
    "    published_time=''\n",
    "    \n",
    "    for meta in metas:\n",
    "        if meta.get('name')=='description':\n",
    "            description=meta.get('content')\n",
    "        if meta.get('name')=='author':\n",
    "            author=meta.get('content')\n",
    "        if meta.get('property')=='article:published_time':\n",
    "            published_time=meta.get('content')\n",
    "    \n",
    "    if article['title'] is None:\n",
    "        article['title']=title\n",
    "    if article['description'] is None:\n",
    "        article['description']=description\n",
    "    if article['author'] is None:\n",
    "        article['author']=author\n",
    "    if article['publishedAt'] is None:\n",
    "        article['publishedAt']=published_time\n",
    "    if article['source']['id'] is None:\n",
    "        source_id_matcher(article)\n",
    "        ## design a seperate finder for news source id to allow more efficient debugging \n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3798f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_category_matcher(article):\n",
    "    \n",
    "    news_sources_names=['ABC News', 'ABC News (AU)', 'Aftenposten', 'Al Jazeera English', 'ANSA.it', 'Argaam', 'Ars Technica', 'Ary News', 'Associated Press', 'Australian Financial Review', 'Axios', 'BBC News', 'BBC Sport', 'Bild', 'Blasting News (BR)', 'Bleacher Report', 'Bloomberg', 'Breitbart News', 'Business Insider', 'Buzzfeed', 'CBC News', 'CBS News', 'CNN', 'CNN Spanish', 'Crypto Coins News', 'Der Tagesspiegel', 'Die Zeit', 'El Mundo', 'Engadget', 'Entertainment Weekly', 'ESPN', 'ESPN Cric Info', 'Financial Post', 'Focus', 'Football Italia', 'Fortune', 'FourFourTwo', 'Fox News', 'Fox Sports', 'Globo', 'Google News', 'Google News (Argentina)', 'Google News (Australia)', 'Google News (Brasil)', 'Google News (Canada)', 'Google News (France)', 'Google News (India)', 'Google News (Israel)', 'Google News (Italy)', 'Google News (Russia)', 'Google News (Saudi Arabia)', 'Google News (UK)', 'Göteborgs-Posten', 'Gruenderszene', 'Hacker News', 'Handelsblatt', 'IGN', 'Il Sole 24 Ore', 'Independent', 'Infobae', 'InfoMoney', 'La Gaceta', 'La Nacion', 'La Repubblica', 'Le Monde', 'Lenta', \"L'equipe\", 'Les Echos', 'Libération', 'Marca', 'Mashable', 'Medical News Today', 'MSNBC', 'MTV News', 'MTV News (UK)', 'National Geographic', 'National Review', 'NBC News', 'News24', 'New Scientist', 'News.com.au', 'Newsweek', 'New York Magazine', 'Next Big Future', 'NFL News', 'NHL News', 'NRK', 'Politico', 'Polygon', 'RBC', 'Recode', 'Reddit /r/all', 'Reuters', 'RT', 'RTE', 'RTL Nieuws', 'SABQ', 'Spiegel Online', 'Svenska Dagbladet', 'T3n', 'TalkSport', 'TechCrunch', 'TechCrunch (CN)', 'TechRadar', 'The American Conservative', 'The Globe And Mail', 'The Hill', 'The Hindu', 'The Huffington Post', 'The Irish Times', 'The Jerusalem Post', 'The Lad Bible', 'The Next Web', 'The Sport Bible', 'The Times of India', 'The Verge', 'The Wall Street Journal', 'The Washington Post', 'The Washington Times', 'Time', 'USA Today', 'Vice News', 'Wired', 'Wired.de', 'Wirtschafts Woche', 'Xinhua Net', 'Ynet']\n",
    "    news_sources_categories=['general', 'general', 'general', 'general', 'general', 'business', 'technology', 'general', 'general', 'business', 'general', 'general', 'sports', 'general', 'general', 'sports', 'business', 'general', 'business', 'entertainment', 'general', 'general', 'general', 'general', 'technology', 'general', 'business', 'general', 'technology', 'entertainment', 'sports', 'sports', 'business', 'general', 'sports', 'business', 'sports', 'general', 'sports', 'general', 'general', 'general', 'general', 'general', 'general', 'general', 'general', 'general', 'general', 'general', 'general', 'general', 'general', 'technology', 'technology', 'business', 'entertainment', 'business', 'general', 'general', 'business', 'general', 'general', 'general', 'general', 'general', 'sports', 'business', 'general', 'sports', 'entertainment', 'health', 'general', 'entertainment', 'entertainment', 'science', 'general', 'general', 'general', 'science', 'general', 'general', 'general', 'science', 'sports', 'sports', 'general', 'general', 'entertainment', 'general', 'technology', 'general', 'general', 'general', 'general', 'general', 'general', 'general', 'general', 'technology', 'sports', 'technology', 'technology', 'technology', 'general', 'general', 'general', 'general', 'general', 'general', 'general', 'entertainment', 'technology', 'sports', 'general', 'technology', 'business', 'general', 'general', 'general', 'general', 'general', 'technology', 'technology', 'business', 'general', 'general']\n",
    "    \n",
    "    if article['source']['name'] in news_sources_names:\n",
    "        idx=news_sources_names.index(article['source']['name'])\n",
    "        category=news_sources_categories[idx]\n",
    "    else:\n",
    "        category=None\n",
    "    \n",
    "    article['category']=category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc8fec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_scalper(article):\n",
    "    response=requests.get(article['url'])\n",
    "    ## check if full article could be retrieved from the website\n",
    "    code=response.status_code\n",
    "    if str(code)[0]!='2':\n",
    "        raise Exception('Access Denied')\n",
    "    \n",
    "    ## using regular expressions and Beautiful Soup to find all non-trivial paragraphs on the page\n",
    "    html=response.text\n",
    "    page=BeautifulSoup(html,'lxml')\n",
    "    paragraphs=page.find_all('p')\n",
    "    pattern='\\w+'\n",
    "    pat=re.compile(pattern)\n",
    "    temp=[]\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        text=paragraph.get_text()\n",
    "        r=pat.findall(text)\n",
    "        if len(r)!=0:\n",
    "            temp.append(text)\n",
    "        \n",
    "    content=' '.join(temp)\n",
    "    ## concatenate paragraphs to form the main content of the article\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad39ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article:\n",
    "    def __init__(self,title,author,publishDate,category,url,source,content):\n",
    "        if type(title) is not str:\n",
    "            title=''\n",
    "        if type(author) is not str:\n",
    "            author=''\n",
    "        if type(publishDate) is not str:\n",
    "            ## some dates might be stored as DateTime or TimeStamp objects\n",
    "            publishDate=str(publishDate)           \n",
    "        if type(category) is not str:\n",
    "            category='n/a'            \n",
    "        if type(source) is not str:\n",
    "            source=str(source)            \n",
    "        if type(content) is not str:\n",
    "            content=''            \n",
    "            \n",
    "        self._title=title\n",
    "        self._author=author\n",
    "        self._publishDate=publishDate\n",
    "        self._category=category\n",
    "        self._link=url\n",
    "        self._source=source\n",
    "        self._content=content\n",
    "        ## content must be a string object\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        ## produce a string that concatenate all information in the article\n",
    "        seperator=' '\n",
    "        result=seperator.join([self._title,self._author,self._publishDate,self._category,self._link,self._source,self._content])\n",
    "        return result\n",
    "    \n",
    "    def display(self):\n",
    "        ## display the article in an easy-to-read fashion\n",
    "        \n",
    "        def _clean_content(content):\n",
    "        ## cleaning the main passage to get more sensible articles\n",
    "            result=re.sub(r'\\s', ' ', content)\n",
    "            return result\n",
    "        \n",
    "        content=_clean_content(self._content)\n",
    "        print(self._title+'\\n')\n",
    "        print(f'Author:{self._author}'+'\\n')\n",
    "        print(f'Published at {self._publishDate} on {self._source}'+'\\n')\n",
    "        print(content+'\\n\\n')\n",
    "        print(f'Link to the original news page: {self._link}')\n",
    "        \n",
    "    def __len__(self):\n",
    "        ## calculate length of the article\n",
    "        seperator=''\n",
    "        result=seperator.join([self._title,self._author,self._publishDate,self._category,self._link,self._source,self._content])\n",
    "        return len(result)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "012f2fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(number=29):\n",
    "    ## optional function: Allow users to set preferences for the number of articles to fetch.\n",
    "    ## non-paid keys can only search up to 100 results\n",
    "    api_key='6adfdd5a094c46daa53ea3bbb74379a5'\n",
    "    api=NewsApiClient(api_key=api_key)\n",
    "    ## use most common words to restrict search(impossible otherwise)\n",
    "    news=api.get_everything(q='the',page=2)\n",
    "\n",
    "    ## randomly select news from sources to get the given number of articles\n",
    "    idx=np.random.randint(low=0,high=len(news['articles'])-1,size=number)\n",
    "    results=[]\n",
    "    for i in idx:\n",
    "        a=news['articles'][i]\n",
    "        results.append(a)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a313f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Topnews:\n",
    "    ## GUI class to get top news with keyword input by the user\n",
    "    ## display aggregate information for each news from search results\n",
    "    def __init__(self):\n",
    "        self.root = tk.Tk()\n",
    "        self.root.geometry('500x400') \n",
    "        self.root.title(\"Today's headline news\")\n",
    "        \n",
    "        self.function_label = tk.Label(self.root, text=\"search and get latest news\")\n",
    "        self.function_label.pack(pady=10)\n",
    "        \n",
    "        self.question_label = tk.Label(self.root, text=\"Please enter your search keywords:\")\n",
    "        self.question_label.pack(pady=10)\n",
    "        self.question_entry = tk.Entry(self.root)\n",
    "        self.question_entry.pack(pady=5)\n",
    "        \n",
    "        self.number_label = tk.Label(self.root, text=\"Number of query(default amount:10 ; maximum amount:100)\")\n",
    "        self.number_label.pack()\n",
    "        self.number_entry = tk.Entry(self.root)\n",
    "        self.number_entry.pack(pady=5)\n",
    "        \n",
    "        self.get_button = tk.Button(self.root, text=\"Get news!\",command=self.get_news)\n",
    "        self.get_button.pack(pady=10)\n",
    "        \n",
    "        self.root.mainloop()\n",
    "        \n",
    "    def get_news(self):\n",
    "        \n",
    "        question=self.question_entry.get()\n",
    "        \n",
    "        try:\n",
    "            Input=int(self.number_entry.get())\n",
    "            if Input>0 and Input<=100:\n",
    "                display_amount=Input\n",
    "            else:\n",
    "                display_amount=10\n",
    "        except:\n",
    "            display_amount=10\n",
    "        \n",
    "        api_key='6adfdd5a094c46daa53ea3bbb74379a5'\n",
    "        api=NewsApiClient(api_key=api_key)\n",
    "        response=api.get_everything(q=question,sort_by='relevancy')\n",
    "        news=response['articles'][:display_amount]\n",
    "        queries=''\n",
    "        for item in news:\n",
    "            t=item['title']\n",
    "            n=item['source']['name']\n",
    "            time=item['publishedAt']\n",
    "            r=f'{t}\\n{n}\\t{time}\\n\\n'\n",
    "            queries+=r\n",
    "        \n",
    "        messagebox.showinfo(title=\"Search Result\",message=queries)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29008f42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
